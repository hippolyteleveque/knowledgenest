{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51de009b-62a0-403f-975d-7b26f3e24ab5",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ba4a081-088f-45b8-96a5-59812b3a9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from getpass import getpass \n",
    "import sys\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable, RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from pinecone.data.index import Index\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5952296-dc11-44d1-9df6-6b86f7bce447",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ace1d845-9150-45df-b731-f2a07a1f5585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89ca3ac2-3e66-4fdf-8ac2-2612cd111b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] =\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"kn-eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bc9b1b5-8776-4bc1-b66e-b7e4ad687740",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78fb3c2f-e86e-42aa-b71a-ab074ad88e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mistral = ChatMistralAI(model=\"open-mistral-nemo\", api_key=MISTRAL_API_KEY)\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5629296-adc1-4b8b-a949-bf2e8aa5c95c",
   "metadata": {},
   "source": [
    "# 3. KN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e80377e-9714-4bfd-8a74-cff5ad80dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add knoledgenest path to allow for imports in the notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory of knowledgenest to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..', '..')))\n",
    "\n",
    "from knowledgenest.vector_database import init_pinecone, EMBEDDING_MODEL_DIM, SIM_METRIC\n",
    "from knowledgenest.chat.utils import KNRag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8df3da85-f1e1-4038-a130-139f779f814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_idx_name = \"knowledgenest-eval\"\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6180983-b60b-4de7-87ca-cc227152f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = init_pinecone(PINECONE_API_KEY, eval_idx_name, EMBEDDING_MODEL_DIM, SIM_METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a9cf5-00c7-4a17-acac-b214639c3ccc",
   "metadata": {},
   "source": [
    "### Ingest evaluation sources for RAG if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af270ed9-850b-442b-b6be-b4c5e9f1f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import embed_and_ingest_article\n",
    "from utils import embed_and_ingest_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c31d9ba-abbb-412c-bcfa-dc7fd13bd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_count = idx.describe_index_stats()[\"total_vector_count\"]\n",
    "is_empty = vector_count == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f5f095d-5127-48fd-97fe-46817a2ed914",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_sources_links = [\n",
    "    \"https://paulgraham.com/foundermode.html\",\n",
    "    \"https://www.paulgraham.com/persistence.html\",\n",
    "    \"https://www.paulgraham.com/reddits.html\",\n",
    "    \"https://www.paulgraham.com/google.html\",\n",
    "    \"https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines\",\n",
    "    \"https://www.palladiummag.com/2024/08/30/when-the-mismanagerial-class-destroys-great-companies/\",\n",
    "]\n",
    "\n",
    "# Collection of short news videos to which the LLM could have had access to when trained. \n",
    "video_sources_links = [\n",
    "    \"https://www.youtube.com/watch?v=8QLVX9A7hqI\",\n",
    "    \"https://www.youtube.com/watch?v=TNc14W8YOuI\",\n",
    "    \"https://www.youtube.com/watch?v=sic0OJyyeZ0\",\n",
    "    \"https://www.youtube.com/watch?v=2HGWuflXCUY\",\n",
    "    \"https://www.youtube.com/watch?v=EDgD7NMY60U\",\n",
    "    \"https://www.youtube.com/watch?v=GUr2AA6ljeU\",\n",
    "    \"https://www.youtube.com/watch?v=DUPH2n3g5bg\",\n",
    "    \"https://www.youtube.com/watch?v=0kOu4GLZRo0\",\n",
    "    \"https://www.youtube.com/watch?v=SsH23u6XiGY\",\n",
    "    \"https://www.youtube.com/watch?v=rvu8N6bA3PI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "645d228c-ea75-4ec1-abf3-83db9f5c6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_empty:\n",
    "    # ingest evaluation sources\n",
    "\n",
    "    # TODO put that in ThreadExecutor\n",
    "    for article_url in articles_sources_links:\n",
    "        embed_and_ingest_article(article_url, idx)\n",
    "    for video_url in video_sources_links:\n",
    "        embed_and_ingest_video(video_url, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "178c9168-71e0-416d-ad18-8d38419e3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "#\n",
    "#if is_empty:\n",
    "#    # Embed articles\n",
    "#    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#        futures = [executor.submit(embed_and_ingest_article, url, idx) for url in articles_sources_links]\n",
    "#        executor.shutdown(wait=True)\n",
    "#\n",
    "#    # Embed videos\n",
    "#    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#        futures = [executor.submit(embed_and_ingest_video, url, idx) for url in video_sources_links]\n",
    "#        executor.shutdown(wait=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d1200-815f-415a-81bc-1994c865333c",
   "metadata": {},
   "source": [
    "# 4. Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23fcf1-6266-490d-a0cd-7db94d5d914e",
   "metadata": {},
   "source": [
    "### A. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "239c3138-29a0-4abe-8cb2-eeb637c940b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10da04c2-845a-4f82-afaa-5181ee7077f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "base_dataset_name = \"kn-eval-qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aafc124b-9a52-461d-a3ec-dc0f151bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QA\n",
    "inputs = [\n",
    "    # First article\n",
    "    \"What is founder mode ?\",\n",
    "\n",
    "    # Second Article\n",
    "    \"What are the main behaviours that set apart persistent from obstinate people ?\",\n",
    "\n",
    "    # Third Article\n",
    "    \"Why did YC did not fund the first idea of Reddit's founders ?\",\n",
    "\n",
    "    # Fourth article\n",
    "    \"What should young people do if they want to start a startup ?\",\n",
    "\n",
    "    # Fifth article\n",
    "    \"What are the responsabilities of the three main components of machine learning systems?\",\n",
    "\n",
    "    # Sixth article\n",
    "    \"Explain the concept of 'portfolio theory of the firm' ?\",\n",
    "\n",
    "    # First video \n",
    "    \"What are the main threats to the Schenghen Area ?\",\n",
    "    \"Which countries are supporting the most the reestablishment of EU national border controls ?\",\n",
    "\n",
    "    # Second video\n",
    "    \"According to Draghi's report, what are the main explanations for Europe's productivity slow down ?\",\n",
    "    \"Give an overview of Draghi's proposed plan to revitalize Europe's economy\",\n",
    "\n",
    "    # Third video\n",
    "    \"On which grounds did president Macron choose its new prime minister ?\",\n",
    "\n",
    "    # Fourth video\n",
    "    \"Why was Spain traditionnaly migrant-friendly ?\",\n",
    "\n",
    "    # Fifth video\n",
    "    \"What are the main driving forces behind the surge of the AFD ?\",\n",
    "\n",
    "    # Sixth video\n",
    "    \"What are the measures that Orban took to revive Hungary's birth rate ?\",\n",
    "\n",
    "    # Seventh video\n",
    "    \"Explain what prevented Belgium to form a goverment ?\",\n",
    "\n",
    "    # Heigth video (can also include information from third video)\n",
    "    \"Why did'nt president Macron nominate a leftwing prime minister\",\n",
    "\n",
    "    # Ninth video\n",
    "    \"What recent events provoked tensions between Germany and Poland ?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    # First Article\n",
    "    (\"According to Paul Graham, founder mode is how company should be ran when they are still led by\"\n",
    "     \"its founders, as opposed as when it's led by professional managers which is the 'manager mode'.\"\n",
    "     \"Founder mode is not very well known, not teached in business school but we know it differs from\"\n",
    "     \"manager mode as lots of founders have tried to mimic manager mode without success, as opposed\"\n",
    "     \"to founders acting differently and achieving great success (as is the case with Steve Jobs)\"),\n",
    "\n",
    "    # Second Article\n",
    "    (\"Persistent people keep listening to others and trying new things and they tend to be more\"\n",
    "     \"focused on the most important things, the overall picture and goal and not too muched\"\n",
    "     \"attached to details\"),\n",
    "\n",
    "    # Third Article\n",
    "    (\"YC did not find the first idea of Steve and Alexis - Reddit's founders - because they thought\"\n",
    "     \"the idea was bad and they were still focusing on funding idea at this time.\"),\n",
    "\n",
    "     # Fourth Article\n",
    "     (\"There are three main things that young people should do in order to optimize their chances\"\n",
    "      \"of founding a successfull startup: learning a technology, follow their interests and build\"\n",
    "      \"projects\"),\n",
    "\n",
    "     # Fifth Article\n",
    "     (\"The three main parts of efficient machine learning systems are the feature pipeline, the training\"\n",
    "      \"pipeline and the inference pipeline. The feature pipeline computes and updates features from the\"\n",
    "      \"data sources, the training pipelines regularly train machine learning systems and version their\"\n",
    "      \"weights, and the inference pipeline is responsible for answering to client requests\"),\n",
    "\n",
    "     # Sixth Article\n",
    "     (\"The portofolio of the firm is the phenomenon by which companies are not treated as human organizations\"\n",
    "      \"but only as a package of financial products which parts can be traded or new parts can be added in order\"\n",
    "      \"to maximize the financial figures of the balance sheet\"),\n",
    "\n",
    "    # First video\n",
    "    \"The main threats to the Schenghen Area is the reestablishment of national bordel control aiming to fight illegal immigration and cross-borer crime\",\n",
    "    (\"The main countries supporting the reestablishment of national border controls are Germany, Poland, Hungary and Denmark.\"\n",
    "     \"Other countries like France could follow suit as well\"),\n",
    "\n",
    "    # Second Video\n",
    "    \"According to Draghi, demographic decline, global markets fragmentation and industrial stagnation accounts for Europe's economical slow down\",\n",
    "    (\"Draghi's proposes three main transformations to revive Europe's economy : \"\n",
    "     \"Invest in new technologies in software and AI\",\n",
    "     \"Invest in decarbonizing the economy\",\n",
    "     \"Diversify supply chains and deepen its internal integration\"),\n",
    "    \n",
    "    # Third Video\n",
    "    (\"Because Barnier is very experienced, especially when it comes to EU politics\",\n",
    "     \"and because the national rally did not oppose him as strongly as others.\"),\n",
    "\n",
    "    # Fourth video\n",
    "    (\"Because of the legacy of Franco's dictatorship that brought skepticism towards\"\n",
    "     \"nationalism, because Spain's welfare state offer very little to immigrants and\"\n",
    "     \"because the Spanish press has been sober on the subject\"),\n",
    "\n",
    "    # Fifth video\n",
    "    (\"The Afd has capitalized on the increasing anti-immigrant sentiment that followed\"\n",
    "     \"the 2010's migration, economic slow down, war in Ukraine and anti-green sentiments\"),\n",
    "\n",
    "    # Sixth video\n",
    "    (\"Family tax reduction, lifetime tax exemption for women with more\"\n",
    "     \"than four children, loans that don't have to be repaid for couple with children\"\n",
    "     \"and housing subsidies.\"),\n",
    "\n",
    "    # Seventh video\n",
    "    (\"Because the different parties could not get along when it comes to fiscal policies\"\n",
    "    \"and because there were lack of trust between the different parties.\"),\n",
    "\n",
    "    \n",
    "    # Heigth\n",
    "    (\"Because a left government would be very influenced by Melenchon's party\"\n",
    "     \"and because it would lack a sustainable majority in Parliament\"),\n",
    "    \n",
    "    # Ninth video\n",
    "    (\"The two main issues are the destruction of the Nord Stream pipeline\"\n",
    "     \"for which Poland is suspected to have played a part and the waning German\"\n",
    "     \"support for the war in Ukraine\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "418f0700-84d7-44f6-ae16-011a7ad1badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = base_dataset_name + \"-v0\"\n",
    "datasets = client.list_datasets(dataset_name=dataset_name)\n",
    "try:\n",
    "    next(datasets) # dataset already exists\n",
    "    print(f\"Dataset {dataset_name} already exists\")\n",
    "    pass\n",
    "except StopIteration:\n",
    "    # dataset does not exist\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Input question of RAG KN\",\n",
    "    )\n",
    "    client.create_examples(\n",
    "        inputs=[{\"question\": q} for q in inputs],\n",
    "        outputs=[{\"answer\": a} for a in outputs],\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cae7-22a2-4133-95eb-ab09ded1b378",
   "metadata": {},
   "source": [
    "### B. Evaluate RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1877835e-fd7e-4b4b-9e50-dccaf0e8aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test the pipeline with mistral\n",
    "provider = \"mistral\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cecdb74-d47f-4f91-8ce3-2dd8c99b56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import trace\n",
    "\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    kn_rag =  KNRag(provider=provider, pc_idx=idx)\n",
    "    message = example[\"question\"]\n",
    "    with trace(\"KNRag\", inputs={\"message\": message}):\n",
    "        response = kn_rag.answer(dict(messages=[HumanMessage(message)]), stream=False)\n",
    "    return {\"answer\": response[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f856f50c-0902-4cc8-827d-a258015827f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_core/utils/utils.py:235: UserWarning: WARNING! seed is not default parameter.\n",
      "                seed was transferred to model_kwargs.\n",
      "                Please confirm that seed is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MistralNemoKNRag-5675a162' at:\n",
      "https://smith.langchain.com/o/a700c4b6-5caf-57dc-a929-900e043ce283/datasets/b0dac92b-d193-4c82-8fa9-5665cc078be0/compare?selectedSessions=ffccee93-3d7e-4a36-acf3-42dea73a92fc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "1it [00:05,  5.33s/it]/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/projects/knowledgenest/backend/venv/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "17it [00:22,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "\n",
    "test_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"MistralNemoKNRag\",\n",
    "    num_repetitions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2e064-5ecd-4f80-9817-b4efb502bc70",
   "metadata": {},
   "source": [
    "### C. CleanUp indexes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba0403f-e4e1-4b09-ba76-a64f398ea5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import delete_index\n",
    "\n",
    "delete_index(PINECONE_API_KEY, eval_idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496384e-5f79-4506-b54d-5d6e2ff98ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgenest",
   "language": "python",
   "name": "knowledgenest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
