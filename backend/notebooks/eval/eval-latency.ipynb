{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51de009b-62a0-403f-975d-7b26f3e24ab5",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ba4a081-088f-45b8-96a5-59812b3a9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from getpass import getpass \n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable, RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from pinecone.data.index import Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5952296-dc11-44d1-9df6-6b86f7bce447",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66392606-a35d-4355-9f7e-5f434cbae1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "LANGCHAIN_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ca3ac2-3e66-4fdf-8ac2-2612cd111b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a413fe-9f83-4789-a9ac-04a3031b0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "MISTRAL_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "481882c6-1776-4890-82c8-105a513bf888",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78fb3c2f-e86e-42aa-b71a-ab074ad88e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mistral = ChatMistralAI(model=\"open-mistral-nemo\", api_key=MISTRAL_API_KEY)\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd1a243d-6712-4a95-adfb-628d1877b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92f7a914-ddc5-4f02-8a80-ab12117d0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_INDEX_NAME = \"knowledgenest-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf794ff1-8bf7-464d-a29b-f0d49d152a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5629296-adc1-4b8b-a949-bf2e8aa5c95c",
   "metadata": {},
   "source": [
    "# 3. KN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbf661-c44f-41f5-a7f1-8d1d1e8ffb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_db():\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    return pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caeec9b-7208-4e56-8087-ab982bac463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: VectorStore, filter: Dict) -> Runnable:\n",
    "    \"\"\"Create and returns a retriever with the specified filters\"\"\"\n",
    "\n",
    "    @chain\n",
    "    def retrieve(query: str) -> List[Document]:\n",
    "        if results := vectorstore.similarity_search_with_score(\n",
    "            query, k=3, filter=filter\n",
    "        ):\n",
    "            docs, scores = zip(*results)\n",
    "            for doc, score in zip(docs, scores):\n",
    "                doc.metadata[\"score\"] = score\n",
    "\n",
    "            return list(docs)\n",
    "        return []\n",
    "\n",
    "    return retrieve\n",
    "\n",
    "\n",
    "def get_chain():\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    index = get_vector_db()\n",
    "    llm = mistral\n",
    "    # We always embed with mistral for index consistency\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "    retriever = create_retriever(vector_store, None)\n",
    "    chain = (\n",
    "        dict(docs=parse_retriever_input | retriever, messages=itemgetter(\"messages\"))\n",
    "        | RunnableParallel(\n",
    "            context=itemgetter(\"docs\") | RunnableLambda(format_docs),\n",
    "            messages=itemgetter(\"messages\"),\n",
    "            sources=itemgetter(\"docs\") | RunnableLambda(parse_sources),\n",
    "        )\n",
    "        | RunnableParallel(\n",
    "            prompt=prompt,\n",
    "            sources=itemgetter(\"sources\"),\n",
    "        )\n",
    "        | RunnableParallel(\n",
    "            output=itemgetter(\"prompt\") | llm | StrOutputParser(),\n",
    "            sources=itemgetter(\"sources\"),\n",
    "        )\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def parse_sources(docs: List[Document]) -> List[Dict]:\n",
    "    \"\"\"Extract unique sources with useful information\"\"\"\n",
    "    sources = {}\n",
    "    for doc in docs:\n",
    "        doc_id = doc.metadata[\"content_id\"]\n",
    "        if doc_id not in sources:\n",
    "            sources[doc_id] = {\n",
    "                \"id\": doc_id,\n",
    "                \"type\": doc.metadata[\"type\"],\n",
    "                \"score\": doc.metadata[\"score\"],\n",
    "            }\n",
    "\n",
    "        elif sources[doc_id][\"score\"] < doc.metadata[\"score\"]:\n",
    "            # We take the best score of each document\n",
    "            sources[doc_id][\"score\"] = doc.metadata[\"score\"]\n",
    "    return list(sources.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93d82094-317c-4a28-be98-80c0712b309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a useful assistant that answers politey to users questions. \n",
    "            Your answers are based on your general knowledge but \n",
    "            you primarily based on the below context when it is useful :\\n\\n{context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ac7774-3e92-46e8-b57a-c06809857dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9155f98-7f69-43ef-96e5-201a27c240a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "914546fe-b9ac-4050-acab-77a05a53e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6baa1261-5424-41f6-b1a5-a6b265cd1b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Hello! How can I assist you today?',\n",
       " 'sources': [{'id': '1da83de6-94ce-4a9c-9d0b-7c70859c718a',\n",
       "   'type': 'article',\n",
       "   'score': 0.637567222},\n",
       "  {'id': '5555acdb-aba5-4d5a-b8d3-e6a1ee24ffde',\n",
       "   'type': 'article',\n",
       "   'score': 0.636965513}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(dict(messages=[HumanMessage(\"helo\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d1200-815f-415a-81bc-1994c865333c",
   "metadata": {},
   "source": [
    "# 4. Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23fcf1-6266-490d-a0cd-7db94d5d914e",
   "metadata": {},
   "source": [
    "### A. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "239c3138-29a0-4abe-8cb2-eeb637c940b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10da04c2-845a-4f82-afaa-5181ee7077f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "base_dataset_name = \"kn-eval-perf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aafc124b-9a52-461d-a3ec-dc0f151bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QA\n",
    "inputs = [\n",
    "    \"What are the challenges when building a real time machine learning system\",\n",
    "    \"What are the different components of the Feature Training Inference architecture ?\",\n",
    "    \"How were machine learning systems built at the beginning ?\",\n",
    "    \"What is the portfolio theory of the firm ?\",\n",
    "    \"What is founder mode ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f0700-84d7-44f6-ae16-011a7ad1badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = base_dataset_name + \"-v2\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Input question of RAG KN\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cae7-22a2-4133-95eb-ab09ded1b378",
   "metadata": {},
   "source": [
    "### B. Evaluate RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cecdb74-d47f-4f91-8ce3-2dd8c99b56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    llm =  get_chain()\n",
    "    response = llm.invoke(dict(messages=[HumanMessage(example[\"question\"])]))\n",
    "    return {\"answer\": response[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f856f50c-0902-4cc8-827d-a258015827f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MistralNemoFullChain-854505a2' at:\n",
      "https://smith.langchain.com/o/a700c4b6-5caf-57dc-a929-900e043ce283/datasets/69a30af2-2443-4053-a943-de470933914e/compare?selectedSessions=0c52b2aa-17c7-4100-8944-62337614a754\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf0258522a243ac98f922b61487535c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n",
      "/Users/hippolyteleveque/Documents/learning_projects/llms/training/env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"MistralNemoFullChain\",\n",
    "    num_repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6264b7f-d536-471b-bdfc-e52a3cb39bf3",
   "metadata": {},
   "source": [
    "### C. Evaluate Simpler chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bf412dd-d94d-4cd7-9804-b73c7405288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_SYSTEM_PROMPT = \"\"\"You are a useful assistant that answers politey to users questions.\"\"\"\n",
    "\n",
    "def get_simple_chain():\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SIMPLE_SYSTEM_PROMPT),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = mistral\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb4d78fa-b722-4fec-bf6d-74d3a762ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "107943f1-1f7e-42fa-804e-dddc0dd644d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nemo_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    llm = get_simple_chain()\n",
    "    response = llm.invoke(dict(messages=[HumanMessage(example[\"question\"])]))\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "faa21225-5adb-434a-b8a0-25371507d45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Building a real-time machine learning system presents several challenges. Here are some of the key ones, along with polite responses to each:\\n\\n1. **Latency and Speed**:\\n   - *Challenge*: Real-time systems require immediate responses, often within milliseconds. This can be challenging, especially for complex models that might take seconds or even minutes to process data.\\n   - *Polite response*: \"I understand that speed is of the essence in real-time systems. We\\'ll need to ensure our model is efficient and perhaps explore techniques like model pruning or hardware acceleration to meet the latency requirements.\"\\n\\n2. **Data Quality and Availability**:\\n   - *Challenge*: Real-time systems often deal with streaming data, which can be noisy, incomplete, or biased. Ensuring the model receives high-quality, representative data can be difficult.\\n   - *Polite response*: \"I appreciate your concern about data quality. We\\'ll need to implement robust data preprocessing and validation steps to ensure our model receives the best possible input.\"\\n\\n3. **Scalability**:\\n   - *Challenge*: Real-time systems must handle varying loads and potentially large amounts of data. Scaling the system to meet these demands can be complex.\\n   - *Polite response*: \"You\\'re right, scalability is crucial. We\\'ll need to design our system with horizontal scaling in mind, perhaps using distributed computing frameworks, to handle increased data load.\"\\n\\n4. **Model Updates and Drift**:\\n   - *Challenge*: Real-world data can change over time, leading to concept drift where the model\\'s assumptions no longer hold. Regular model updates are necessary, but they can disrupt the system\\'s real-time operation.\\n   - *Polite response*: \"I agree, concept drift is a significant challenge. We\\'ll need to implement online learning or periodic retraining to adapt to changing data patterns without interrupting the system\\'s real-time capabilities.\"\\n\\n5. **Resource Constraints**:\\n   - *Challenge*: Real-time systems often run on resource-constrained devices, such as edge devices or mobile phones, which may not have the processing power or memory to run complex models.\\n   - *Polite response*: \"I understand the resource constraints we\\'re working with. We\\'ll need to consider model compression techniques or use lightweight models to ensure our system runs smoothly on these devices.\"\\n\\n6. **Explainability and Interpretability**:\\n   - *Challenge*: Real-time systems often need to explain their decisions, especially in critical applications. However, real-time models are often complex and not easily interpretable.\\n   - *Polite response*: \"You\\'re correct, explainability is important. We\\'ll need to consider using interpretable models or techniques like LIME or SHAP to explain our model\\'s decisions in real-time.\"\\n\\n7. **Security and Privacy**:\\n   - *Challenge*: Real-time systems often handle sensitive data, making security and privacy a significant concern.\\n   - *Polite response*: \"I agree, security and privacy are paramount. We\\'ll need to implement robust security measures and consider privacy-preserving techniques, like federated learning or differential privacy, to protect user data.\"\\n\\nEach of these challenges requires careful consideration and often a combination of techniques to address effectively.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nemo_answer(dict(question=inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b59b48d-a1d7-41f1-913e-ade97ba83271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MistralNemoSolo-bcf1075c' at:\n",
      "https://smith.langchain.com/o/a700c4b6-5caf-57dc-a929-900e043ce283/datasets/69a30af2-2443-4053-a943-de470933914e/compare?selectedSessions=18d0e55b-22b0-4e10-807f-6997cbc16851\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20535e3a3c14625bef0f518f3394182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = evaluate(\n",
    "    predict_nemo_answer,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"MistralNemoSolo\",\n",
    "    num_repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fee78e-b5e9-4f69-8736-7af2dd88e41b",
   "metadata": {},
   "source": [
    "### D. Evaluate changes in rag chain code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "923edd7e-f8aa-46e8-a84d-d54fe9d56494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.run_helpers import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75eb066d-0646-4d54-923e-b43c1693fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_v2():\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    index = get_vector_db()\n",
    "    llm = mistral\n",
    "    # We always embed with mistral for index consistency\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "    retriever = create_retriever(vector_store, None)\n",
    "    retriever_chain = parse_retriever_input | retriever\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    @traceable()\n",
    "    def kn_rag(params: dict):\n",
    "        docs = retriever_chain.invoke(params)\n",
    "        sources = parse_sources(docs)\n",
    "        formatted_docs = format_docs(docs)\n",
    "        output = llm_chain.invoke(dict(**params, context=formatted_docs))\n",
    "        return sources, output\n",
    "\n",
    "    return kn_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fb85648-e9d0-4c50-94e2-18a58634cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_chain_v2()\n",
    "response = llm.invoke(dict(messages=[HumanMessage(inputs[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50dce08b-2fb6-4156-8ac3-6c6bbcd3661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'id': '5db351c7-984b-4c3c-bc90-6d3cc0013d63',\n",
       "   'type': 'video',\n",
       "   'score': 0.864119709},\n",
       "  {'id': 'a0790e62-19d8-4341-8f2d-e46ee6934f31',\n",
       "   'type': 'article',\n",
       "   'score': 0.859660447}],\n",
       " 'Based on the context provided, here are some key challenges when building a real-time machine learning system:\\n\\n1. **Data Streams and Ingestion:**\\n   - Handling continuous, real-time data streams efficiently.\\n   - Ingesting and preprocessing data in real-time, which can be challenging due to the volume, velocity, and variety of data.\\n   - Ensuring data quality and consistency in real-time.\\n\\n2. **Model Serving:**\\n   - Deploying and serving models in real-time with low latency.\\n   - Scaling the model serving infrastructure to handle increased load.\\n   - Ensuring high availability and fault tolerance of the model serving system.\\n\\n3. **Feature Store and Context:**\\n   - Managing and serving features in real-time, especially when historical or contextual data is required.\\n   - Ensuring consistency between features used for training and those used for serving, especially in a microservices architecture.\\n\\n4. **Model Updates and Retraining:**\\n   - Updating and retraining models in real-time or near real-time to adapt to changing data patterns.\\n   - Balancing the trade-off between model freshness and serving latency.\\n   - Ensuring smooth transitions between old and new models with minimal downtime.\\n\\n5. **Monitoring and Feedback Loop:**\\n   - Monitoring the performance of the real-time system, including data ingestion, feature computation, model serving, and predictions.\\n   - Establishing a feedback loop to continuously improve the system based on real-time monitoring data.\\n\\n6. **Handling Cold Starts:**\\n   - Dealing with situations where the system has to make predictions with limited or no historical data (cold start problem).\\n\\n7. **Data Privacy and Security:**\\n   - Ensuring data privacy and security in real-time systems, especially when dealing with sensitive data.\\n\\n8. **Complex Infrastructure:**\\n   - Managing the complex infrastructure required for real-time systems, including data pipelines, message queues, streaming platforms, and model serving platforms.\\n\\n9. **Cost and Resource Optimization:**\\n   - Optimizing costs and resource usage in real-time systems, which can be resource-intensive due to the need for low latency and high throughput.\\n\\n10. **Testing and Deployment:**\\n    - Testing real-time systems can be challenging due to their dynamic nature.\\n    - Deploying and managing real-time systems in production environments can also be complex.')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "532005a5-3450-42ec-907b-29d96f8d6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clean_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    llm = get_chain_v2()\n",
    "    response = llm.invoke(dict(messages=[HumanMessage(example[\"question\"])]))\n",
    "    return {\"answer\": response[1]} # get only the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c9242d5-36c5-4076-ab3f-fef7497d3d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MistralNemoRagClean-b783bd59' at:\n",
      "https://smith.langchain.com/o/a700c4b6-5caf-57dc-a929-900e043ce283/datasets/69a30af2-2443-4053-a943-de470933914e/compare?selectedSessions=84d82ff6-299f-439f-8582-ffb3fd84a83c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd851b0c433a4385853433f4990fb6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate(\n",
    "    predict_clean_rag_answer,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"MistralNemoRagClean\",\n",
    "    num_repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b421a8-6afb-49c0-bd1f-63f64bb46547",
   "metadata": {},
   "source": [
    "### E. Put the chain in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee31226-bbd6-4a7d-b40c-cfa6b4878c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea6839a8-c112-42ca-b56b-136d68434bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNRag:\n",
    "\n",
    "    def __init__(self):\n",
    "        #self._user_id = user_id\n",
    "        #self._provider = provider\n",
    "        self._retriever = self._init_retriever()\n",
    "        self._llm = self._init_llm()\n",
    "\n",
    "    @traceable()\n",
    "    def answer(self, params: dict):\n",
    "        docs = self._retriever.invoke(params)\n",
    "        sources = parse_sources(docs)\n",
    "        formatted_docs = format_docs(docs)\n",
    "        output = self._llm.invoke(dict(**params, context=formatted_docs))\n",
    "        return {\n",
    "            \"sources\": sources,\n",
    "            \"output\": output\n",
    "        }\n",
    "\n",
    "    def _init_retriever(self):\n",
    "        index = get_vector_db()\n",
    "        embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "        vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "        retriever = self._create_retriever(vector_store, None)\n",
    "        \n",
    "        retriever_chain = parse_retriever_input | retriever\n",
    "        return retriever_chain\n",
    "\n",
    "    def _init_llm(self):\n",
    "        llm = mistral\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "        llm_chain = prompt | llm | StrOutputParser()\n",
    "        return llm_chain\n",
    "\n",
    "    def _format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "    def _parse_retriever_input(self, params: Dict):\n",
    "        return params[\"messages\"][-1].content\n",
    "    \n",
    "    \n",
    "    def _parse_sources(self, docs: List[Document]) -> List[Dict]:\n",
    "        \"\"\"Extract unique sources with useful information\"\"\"\n",
    "        sources = {}\n",
    "        for doc in docs:\n",
    "            doc_id = doc.metadata[\"content_id\"]\n",
    "            if doc_id not in sources:\n",
    "                sources[doc_id] = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"type\": doc.metadata[\"type\"],\n",
    "                    \"score\": doc.metadata[\"score\"],\n",
    "                }\n",
    "    \n",
    "            elif sources[doc_id][\"score\"] < doc.metadata[\"score\"]:\n",
    "                # We take the best score of each document\n",
    "                sources[doc_id][\"score\"] = doc.metadata[\"score\"]\n",
    "        return list(sources.values())\n",
    "    \n",
    "    def _create_retriever(self, vectorstore: VectorStore, filter: Dict) -> Runnable:\n",
    "        \"\"\"Create and returns a retriever with the specified filters\"\"\"\n",
    "    \n",
    "        @chain\n",
    "        def retrieve(query: str) -> List[Document]:\n",
    "            if results := vectorstore.similarity_search_with_score(\n",
    "                query, k=3, filter=filter\n",
    "            ):\n",
    "                docs, scores = zip(*results)\n",
    "                for doc, score in zip(docs, scores):\n",
    "                    doc.metadata[\"score\"] = score\n",
    "    \n",
    "                return list(docs)\n",
    "            return []\n",
    "    \n",
    "        return retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ff535dd8-ea74-48e3-bb7d-bce9c4ed5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = KNRag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7b477f8-8a11-46fd-b507-704ae9cc5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chatbot.answer(dict(messages=[HumanMessage(inputs[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74c78f54-9a58-4df3-969b-f3e89bd962bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [{'id': '5db351c7-984b-4c3c-bc90-6d3cc0013d63',\n",
       "   'type': 'video',\n",
       "   'score': 0.864119709},\n",
       "  {'id': 'a0790e62-19d8-4341-8f2d-e46ee6934f31',\n",
       "   'type': 'article',\n",
       "   'score': 0.859660447}],\n",
       " 'output': \"Based on the provided context, here are the key challenges when building a real-time machine learning system:\\n\\n1. **Separation of Training and Serving**: Real-time systems require separate pipelines for offline training and online model serving. This separation can make it challenging to ensure consistent features between training and serving.\\n\\n2. **Feature Management**: In real-time systems, features are often computed using data provided in the request. This means the model has no access to historical or contextual data. Ensuring that the features used for training and serving are consistent can be complex. One solution is to version the feature creation source code and use the same version in both training and serving.\\n\\n3. **Latency and Throughput**: Real-time systems need to process data and make predictions quickly. This requires careful management of system resources to ensure low latency and high throughput.\\n\\n4. **Data Freshness**: Real-time systems often rely on very fresh or recent data to make predictions. This can add complexity to data ingestion and preprocessing pipelines.\\n\\n5. **Scalability**: As the volume of data and requests grows, the system needs to be able to scale to handle the increased load. This can involve distributed computing, auto-scaling, and other techniques.\\n\\n6. **Monitoring and Feedback Loop**: Real-time systems require continuous monitoring to ensure they are performing as expected. This includes monitoring the quality of the predictions, the health of the system, and the data it's processing. A feedback loop is often needed to retrain models and improve performance over time.\\n\\n7. **Handling Concept Drift**: Real-time systems may encounter concept drift, where the statistical properties of the input data change over time. This can cause the model's performance to degrade, and the system needs to be able to detect and adapt to these changes.\\n\\n8. **Handling Cold Start Problem**: Real-time systems may encounter situations where they need to make predictions with very little or no historical data available, known as the cold start problem. This can be challenging to handle, especially for models that rely on historical data to make accurate predictions.\\n\\n9. **Ensuring Data Privacy and Security**: Real-time systems often handle sensitive data, so it's crucial to ensure that the system is secure and respects user privacy. This can add additional complexity to the system design and implementation.\\n\\n10. **Cost Management**: Real-time systems can be resource-intensive, and managing costs can be a challenge, especially in cloud environments where resources are paid for on a usage basis.\"}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "52cd70af-8878-465b-a234-c0bd6804d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    llm = KNRag()\n",
    "    response = llm.answer(dict(messages=[HumanMessage(example[\"question\"])]))\n",
    "    return {\"answer\": response[\"output\"]} # get only the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96787411-bc19-4bbd-b9f8-cb1aa2bf0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MistralNemoRagClass-77d5617b' at:\n",
      "https://smith.langchain.com/o/a700c4b6-5caf-57dc-a929-900e043ce283/datasets/69a30af2-2443-4053-a943-de470933914e/compare?selectedSessions=77ed51fe-f03f-4259-a4e5-fc070ca6850c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2b9ce652e04793b78904fdcf7999f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n",
      "An error occurred with MistralAI: 'data'\n",
      "Error running target function: 'data'\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate(\n",
    "    predict_class_rag_answer,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"MistralNemoRagClass\",\n",
    "    num_repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f40d29-9d3f-4712-8e03-5624794685b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-stack",
   "language": "python",
   "name": "langchain-stack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
